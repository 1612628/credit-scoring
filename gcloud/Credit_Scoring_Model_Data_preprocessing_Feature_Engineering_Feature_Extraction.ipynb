{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "fmYMAvvVsIOP",
    "outputId": "2091bfa1-cf64-40fa-d769-3a652869d46e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.7/site-packages (1.1.1)\n",
      "Collecting ipython-autotime\n",
      "  Downloading ipython-autotime-0.1.tar.bz2 (1.2 kB)\n",
      "Building wheels for collected packages: ipython-autotime\n",
      "  Building wheel for ipython-autotime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ipython-autotime: filename=ipython_autotime-0.1-py3-none-any.whl size=1830 sha256=c9666f371998367cd5caa7f12dd48ca1eb0c6d9fe30ea408235fe3be7ca5050f\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/65/56/4a/4b967e4b9b62bd9d8d7ca789bba648c702d705487f28845bb2\n",
      "Successfully built ipython-autotime\n",
      "Installing collected packages: ipython-autotime\n",
      "Successfully installed ipython-autotime-0.1\n",
      "Requirement already satisfied: scorecardpy in /opt/conda/lib/python3.7/site-packages (0.1.9.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scorecardpy) (0.22.2.post1)\n",
      "Requirement already satisfied: pandas>=0.25.0 in /opt/conda/lib/python3.7/site-packages (from scorecardpy) (1.0.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from scorecardpy) (3.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from scorecardpy) (1.18.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->scorecardpy) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->scorecardpy) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25.0->scorecardpy) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25.0->scorecardpy) (2019.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->scorecardpy) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->scorecardpy) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->scorecardpy) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.25.0->scorecardpy) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->scorecardpy) (46.0.0.post20200311)\n"
     ]
    }
   ],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Garbage collector\n",
    "import gc\n",
    "\n",
    "!pip3 install unidecode\n",
    "!pip3 install ipython-autotime\n",
    "!pip3 install scorecardpy\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vJ6kEDIhsNGo",
    "outputId": "5d8b8c6b-2ba7-4f69-e9ba-153371ad714b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 229 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "def pca(train, test):\n",
    "    pca = PCA(n_components=0.97, svd_solver='full')\n",
    "    train_pca = pd.DataFrame(pca.fit_transform(train), columns = ['pca_' + str(i) for i in range(pca.n_components_)])\n",
    "    test_pca = pd.DataFrame(pca.transform(test), columns = ['pca_' + str(i) for i in range(pca.n_components_)])\n",
    "    return (train_pca, test_pca)\n",
    "\n",
    "def one_hot_encoding(X_train, X_test):\n",
    "    train = X_train.copy()\n",
    "    test = X_test.copy()\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    train_ohe = pd.DataFrame()\n",
    "    test_ohe = pd.DataFrame()\n",
    "\n",
    "    for fea in train:\n",
    "        train[fea] = train[fea].replace(to_replace=[np.nan], value='none')\n",
    "        test[fea] = test[fea].replace(to_replace=[np.nan], value='none')\n",
    "\n",
    "        temp_train = enc.fit_transform(train[fea].values.reshape(-1,1)).toarray()\n",
    "        temp_test = enc.transform(test[fea].values.reshape(-1,1)).toarray()\n",
    "\n",
    "        train_ohe = pd.concat([train_ohe, pd.DataFrame(temp_train, columns=[fea + '_ohe_' + str(enc.categories_[0][i]) for i in range(len(enc.categories_[0]))])], axis=1)\n",
    "        test_ohe = pd.concat([test_ohe, pd.DataFrame(temp_test, columns=[fea + '_ohe_' + str(enc.categories_[0][i]) for i in range(len(enc.categories_[0]))])], axis=1)\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "    return (train_ohe, test_ohe)\n",
    "\n",
    "def label_encoding(X_train, X_test):\n",
    "    train = X_train.copy()\n",
    "    test = X_test.copy()\n",
    "    \n",
    "    train_label = pd.DataFrame()\n",
    "    test_label = pd.DataFrame()\n",
    "\n",
    "    for fea in train:\n",
    "        train[fea] = train[fea].replace(to_replace=[np.nan], value='none')\n",
    "        test[fea] = test[fea].replace(to_replace=[np.nan], value='none')\n",
    "\n",
    "        factorised = pd.factorize(train[fea])[1]\n",
    "        labels = pd.Series(range(len(factorised)), index=factorised)\n",
    "\n",
    "        temp_train = train[fea].map(labels)\n",
    "        temp_test = test[fea].map(labels)\n",
    "\n",
    "        train_label[fea + '_labeled'] = temp_train\n",
    "        test_label[fea + '_labeled'] = temp_test\n",
    "\n",
    "    train_label.fillna(-1, inplace=True)\n",
    "    test_label.fillna(-1, inplace=True)\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "    return (train_label, test_label)\n",
    "\n",
    "def freq_encoding(X_train, X_test):\n",
    "    train = X_train.copy()\n",
    "    test = X_test.copy()\n",
    "    \n",
    "    encoded_train_cols = dict()\n",
    "    encoded_test_cols = dict()\n",
    "    for col in train:\n",
    "        train[col] = train[col].replace(to_replace=[np.nan], value='none')\n",
    "        test[col] = test[col].replace(to_replace=[np.nan], value='none')\n",
    "\n",
    "        freq_cats = train.groupby([col])[col].count()/train.shape[0]\n",
    "        encoded_train_cols[str(col) + '_freq'] = train[col].map(freq_cats)\n",
    "        encoded_test_cols[str(col) + '_freq'] = test[col].map(freq_cats)\n",
    "\n",
    "    encoded_train_cols = pd.DataFrame(encoded_train_cols)\n",
    "    encoded_train_cols.fillna(0, inplace=True)\n",
    "    encoded_test_cols = pd.DataFrame(encoded_test_cols)\n",
    "    encoded_test_cols.fillna(0, inplace=True)\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "    return (encoded_train_cols, encoded_test_cols)\n",
    "\n",
    "\n",
    "def mean_encoding(X_train, X_test, target, alpha=0, folds=5, random=True, random_state=913100):\n",
    "    \n",
    "    train = pd.concat([X_train, target], axis=1)\n",
    "    test = X_test.copy()\n",
    "    encoded_train_cols = dict()\n",
    "    encoded_test_cols = dict()\n",
    "    target_mean_gobal = train[target.name].mean()\n",
    "    \n",
    "    for col in X_train:\n",
    "      train[col] = train[col].replace(to_replace=[np.nan], value='none')\n",
    "      test[col] = test[col].replace(to_replace=[np.nan], value='none')\n",
    "\n",
    "      # Getting mean for test data\n",
    "      groups = train.groupby([col])\n",
    "      nrows_cat = groups[target.name].count()\n",
    "      target_mean_cats = groups[target.name].mean()\n",
    "      target_mean_cats_adj = (target_mean_cats*nrows_cat + target_mean_gobal*alpha) / (nrows_cat + alpha) \n",
    "      # Mapping mean to test data\n",
    "      encoded_test_cols[str(col) + '_mean'] = test[col].map(target_mean_cats_adj)\n",
    "\n",
    "      if folds is None:\n",
    "        encoded_train_cols[str(col) + '_mean'] = train[col].map(target_mean_cats_adj)\n",
    "      else:\n",
    "        kfold = StratifiedKFold(n_splits=folds, shuffle=random, random_state=random_state)  \n",
    "        parts = []\n",
    "        # Kfold for train data\n",
    "        for tr_idx, dev_idx in kfold.split(train.drop(columns=target.name), train[target.name]):\n",
    "            # Divide data\n",
    "            base_df, estimate_df = train.iloc[tr_idx], train.iloc[dev_idx]\n",
    "\n",
    "            # Gettting mean of base_df for estimation\n",
    "            groups = base_df.groupby([col])\n",
    "            nrows_cat = groups[target.name].count()\n",
    "            target_mean_cats = groups[target.name].mean()\n",
    "            target_mean_cats_adj = (target_mean_cats*nrows_cat + target_mean_gobal*alpha) / (nrows_cat + alpha) \n",
    "            # Mapping mran for estimate_df\n",
    "            parts.extend(estimate_df[col].map(target_mean_cats_adj))\n",
    "\n",
    "        encoded_train_cols[str(col)+ '_mean'] = parts\n",
    "  \n",
    "    encoded_train_cols = pd.DataFrame(encoded_train_cols)\n",
    "    encoded_train_cols.fillna(target_mean_gobal, inplace=True)\n",
    "    \n",
    "    encoded_test_cols = pd.DataFrame(encoded_test_cols)\n",
    "    encoded_test_cols.fillna(target_mean_gobal, inplace=True)\n",
    "    del train, test\n",
    "    gc.collect() \n",
    "    return (encoded_train_cols, encoded_test_cols)\n",
    "\n",
    "def scoring_ngboost_clf(X_train, y_train, X_dev, y_dev, random_state=913100, verbose=False):\n",
    "    iterations = []\n",
    "    train_scores = []\n",
    "    dev_scores = []\n",
    "  \n",
    "  \n",
    "    log_iters = list(set((np.logspace(math.log(1, 8), math.log(500, 8), \n",
    "                                        num=50, endpoint=True, base=8, \n",
    "                                        dtype=np.int))))\n",
    "    for estimators in sorted(log_iters):\n",
    "        model = MyNGBClassifier(n_estimators=estimators, random_state=random_state)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred_scores = model.predict_proba(X_train)\n",
    "        y_dev_pred_scores = model.predict_proba(X_dev)\n",
    "\n",
    "        train_scores.append(roc_auc_score(y_train, y_train_pred_scores[:, 1]))\n",
    "        dev_scores.append(roc_auc_score(y_dev, y_dev_pred_scores[:, 1]))\n",
    "        iterations.append(estimators)\n",
    "        if verbose:\n",
    "            print(f'{iterations[-1]}/{len(log_iters)}', train_scores[-1], dev_scores[-1])\n",
    "  \n",
    "    best_score = max(dev_scores)\n",
    "    best_iter = iterations[dev_scores.index(best_score)]\n",
    "    if verbose:\n",
    "        print(f'Best score: {best_score}. Best iter: {best_iter}')\n",
    "    return (train_scores, dev_scores, iterations, model)\n",
    "\n",
    "def test_all_encodings(train, dev, target_name):\n",
    "    # Format: encoding function, encoding params, encoding name, encoding color\n",
    "    encoding_settings = [\n",
    "                      [one_hot_encoding, {}, 'One hot encoding', '#E7E005'],\n",
    "                      [label_encoding, {}, 'Label encoding', '#960000'],\n",
    "                      [freq_encoding, {}, 'Frequency encoding', '#FF2F02'],\n",
    "                      [mean_encoding, {'alpha':0, 'folds':None, 'target':train['label']}, 'Mean encoding, alpha=0', '#A4C400'],\n",
    "                      [mean_encoding, {'alpha':2, 'folds':None, 'target':train['label']}, 'Mean encoding, alpha=2', '#73B100'],\n",
    "                      [mean_encoding, {'alpha':5, 'folds':None, 'target':train['label']}, 'Mean encoding, alpha=5', '#2B8E00'],\n",
    "                      [mean_encoding, {'alpha':5, 'folds':3, 'target':train['label']}, 'Mean encoding, alpha=5, 3 folds', '#00F5F2'],\n",
    "                      [mean_encoding, {'alpha':5, 'folds':5, 'target':train['label']}, 'Mean encoding, alpha=5, 5 folds', '#00BAD3'],\n",
    "    ]\n",
    "    scoring_func = scoring_ngboost_clf\n",
    "    plt.figure(figsize=(10,7))\n",
    "\n",
    "    review_rows = []\n",
    "\n",
    "    for encoding_func, encoding_params, str_name, color in encoding_settings:\n",
    "        print(str_name)\n",
    "        X_train, X_dev = encoding_func(train.drop(columns=target_name), dev.drop(columns=target_name), **encoding_params)\n",
    "\n",
    "        # X_train_pca, X_dev_pca = pca(X_train, X_dev)\n",
    "\n",
    "        scores = scoring_func(X_train, train[target_name], X_dev, dev[target_name])\n",
    "\n",
    "        train_scores, dev_scores, iters, _ = scores\n",
    "        plt.plot(iters,  dev_scores,  label='Test, ' + str_name, linewidth=1.5, color=color)\n",
    "\n",
    "        best_score_dev = max(dev_scores)\n",
    "        best_iter_dev = iters[dev_scores.index(best_score_dev)]\n",
    "        best_score_train = max(train_scores[:best_iter_dev])\n",
    "\n",
    "        print(f'Best score for {str_name} is {best_score_dev}, on estimators {best_iter_dev}')\n",
    "        review_rows.append([str_name, best_score_train, best_score_dev, best_iter_dev])\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    columns = ['Encoding', 'Train AUC score on best iteration', 'Best AUC score (test)', 'Best iteration (test)']\n",
    "    return pd.DataFrame(review_rows, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IbYc0FhpsSuy",
    "outputId": "6c0d2e43-3c90-4649-f316-70981aac50fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 399 ms\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./input/train_input_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Nydc0kZm5q8W",
    "outputId": "af0f8f05-181d-4825-d7d2-08d34ded6fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "kfold = []\n",
    "for i in range(0,5):\n",
    "  X_train = pd.read_csv(\"./cv_input/X_train_{}.csv\".format(i + 1))\n",
    "  y_train = pd.read_csv(\"./cv_input/y_train_{}.csv\".format(i + 1))\n",
    "  X_dev = pd.read_csv(\"./cv_input/X_dev_{}.csv\".format(i + 1))\n",
    "  y_dev = pd.read_csv(\"./cv_input/y_dev_{}.csv\".format(i + 1))\n",
    "  kfold.append({\n",
    "      \"X_train\": X_train,\n",
    "      \"y_train\": y_train['label'],\n",
    "      \"X_dev\": X_dev,\n",
    "      \"y_dev\": y_dev['label'],\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "ExpAuDPdpha8",
    "outputId": "1aca68c8-f714-47fa-ac3c-c7fb73ffb83c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 63)\n",
      "(6000, 63)\n",
      "(24000, 63)\n",
      "(6000, 63)\n",
      "(24000, 63)\n",
      "(6000, 63)\n",
      "(24000, 63)\n",
      "(6000, 63)\n",
      "(24000, 63)\n",
      "(6000, 63)\n",
      "time: 1.29 ms\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "  print(kfold[i]['X_train'].shape)\n",
    "  print(kfold[i]['X_dev'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hc3Ee0GHs24K"
   },
   "source": [
    "# Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "HA2GeV_3s6bd",
    "outputId": "c8496fc9-2dfb-4095-972a-e5fe8a17c3f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['province',\n",
       " 'district',\n",
       " 'maCv',\n",
       " 'FIELD_8',\n",
       " 'FIELD_9',\n",
       " 'FIELD_10',\n",
       " 'FIELD_13',\n",
       " 'FIELD_35',\n",
       " 'FIELD_39',\n",
       " 'FIELD_41',\n",
       " 'FIELD_42',\n",
       " 'FIELD_44']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.29 ms\n"
     ]
    }
   ],
   "source": [
    "cat_features = [col for col in train if train[col].dtype == 'object']\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_f-XlpX5-Av"
   },
   "source": [
    "## KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Bo9gk9Uz5_i-",
    "outputId": "5480aeee-ce49-4032-c3ba-5f40a7c610dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "# # Transform FIELD_10 FIELD_13 FIELD_39\n",
    "from unidecode import unidecode\n",
    "from scipy.stats import chi2_contingency\n",
    "train_cat_fea_engineer_combines = []\n",
    "test_cat_fea_engineer_combines = []\n",
    "for i in range(len(kfold)):\n",
    "  for feature in ['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']:\n",
    "    kfold[i]['X_train'][feature].replace(to_replace=[np.nan], value='none', inplace=True)\n",
    "    kfold[i]['X_train'][feature] = kfold[i]['X_train'][feature].apply(unidecode).apply(str.lower)\n",
    "\n",
    "    kfold[i]['X_dev'][feature].replace(to_replace=[np.nan], value='none', inplace=True)\n",
    "    kfold[i]['X_dev'][feature] = kfold[i]['X_dev'][feature].apply(unidecode).apply(str.lower)\n",
    "\n",
    "  train_cat_ohe, test_cat_ohe = one_hot_encoding(kfold[i]['X_train'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
    "                                                 kfold[i]['X_dev'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']])\n",
    "\n",
    "\n",
    "  train_cat_label, test_cat_label = label_encoding(kfold[i]['X_train'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
    "                                                  kfold[i]['X_dev'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']])\n",
    "\n",
    "  train_cat_feq, test_cat_feq = freq_encoding(kfold[i]['X_train'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
    "                                              kfold[i]['X_dev'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']])\n",
    "\n",
    "  encoding_params =  {'alpha':5, 'folds':3, 'target':kfold[i]['y_train']}\n",
    "  train_cat_mean, test_cat_mean = mean_encoding(kfold[i]['X_train'][['province', 'FIELD_8', 'FIELD_9', 'FIELD_10', 'FIELD_13', 'FIELD_35', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
    "                                                kfold[i]['X_dev'][['province', 'FIELD_8', 'FIELD_9', 'FIELD_10', 'FIELD_13', 'FIELD_35', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
    "                                                **encoding_params) \n",
    "\n",
    "\n",
    "  train_cat_fea_engineer_combine = pd.concat([train_cat_ohe, train_cat_label, train_cat_feq, train_cat_mean], axis=1)\n",
    "  test_cat_fea_engineer_combine = pd.concat([test_cat_ohe, test_cat_label, test_cat_feq, test_cat_mean], axis=1)\n",
    "\n",
    "  \n",
    "\n",
    "  del train_cat_ohe, train_cat_label, train_cat_feq, train_cat_mean, test_cat_ohe, test_cat_label, test_cat_feq, test_cat_mean\n",
    "  gc.collect()\n",
    "\n",
    "  cat_fea_engineer_combine_selected_columns = []\n",
    "  for fea in train_cat_fea_engineer_combine:\n",
    "    props = pd.crosstab(train_cat_fea_engineer_combine[fea], kfold[i]['y_train'])\n",
    "    c = chi2_contingency(props, lambda_='log-likelihood')\n",
    "    if (c[1] <= 0.05):\n",
    "      cat_fea_engineer_combine_selected_columns.append(fea)\n",
    "    # print(f'{props} \\np-value={c[1]}\\n')\n",
    "  train_cat_fea_engineer_combine = train_cat_fea_engineer_combine[cat_fea_engineer_combine_selected_columns]\n",
    "  test_cat_fea_engineer_combine = test_cat_fea_engineer_combine[cat_fea_engineer_combine_selected_columns] \n",
    "\n",
    "  train_cat_fea_engineer_combines.append(train_cat_fea_engineer_combine)\n",
    "  test_cat_fea_engineer_combines.append(test_cat_fea_engineer_combine)\n",
    "  # kfold[i]['X_train'] = pd.concat([kfold[i]['X_train'], train_cat_fea_engineer_combines[cat_fea_engineer_combine_selected_columns]], axis=1)\n",
    "  # kfold[i]['X_dev'] = pd.concat([kfold[i]['X_dev'], train_cat_fea_engineer_combines[cat_fea_engineer_combine_selected_columns]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sji9LuCj6kF0"
   },
   "source": [
    "# Add Subtract Divide Multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oErhJQWxLs88"
   },
   "source": [
    "## KFOLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XNJ_D6D26oPK",
    "outputId": "ea5abef2-d890-4278-86fb-b4e4c9f7c92c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17 s\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "auto_columns = 'FIELD_1 FIELD_2 FIELD_3 FIELD_4 FIELD_5 FIELD_6 FIELD_14 FIELD_15 FIELD_16 FIELD_21 FIELD_22 FIELD_32 FIELD_33 FIELD_34 FIELD_46 FIELD_50 \\\n",
    "FIELD_51 FIELD_52 FIELD_53 FIELD_54 FIELD_55 FIELD_56 FIELD_57'.split()\n",
    "train_num_fea_engineer_combines = []\n",
    "test_num_fea_engineer_combines = []\n",
    "for i in range(len(kfold)):\n",
    "  train_num_fea_engineer_combine = pd.DataFrame()\n",
    "  test_num_fea_engineer_combine = pd.DataFrame()\n",
    "  for l, r in combinations(auto_columns, 2):  \n",
    "    for func in 'add subtract divide multiply'.split():\n",
    "      train_num_fea_engineer_combine[f'auto_{func}_{l}_{r}'] = getattr(np, func)(kfold[i]['X_train'][l], kfold[i]['X_train'][r])\n",
    "      test_num_fea_engineer_combine[f'auto_{func}_{l}_{r}'] = getattr(np, func)(kfold[i]['X_dev'][l], kfold[i]['X_dev'][r])\n",
    "  \n",
    "  train_num_fea_engineer_combine.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "  train_num_fea_engineer_combine.fillna(-999, inplace=True)\n",
    "\n",
    "  test_num_fea_engineer_combine.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "  test_num_fea_engineer_combine.fillna(-999, inplace=True)\n",
    "  \n",
    "\n",
    "  # num_fea_engineer_combine_selected_columns = []\n",
    "  # for fea in train_num_fea_engineer_combine:\n",
    "  #   props = pd.crosstab(train_num_fea_engineer_combine[fea], kfold[i]['y_train'])\n",
    "  #   c = chi2_contingency(props, lambda_='log-likelihood')\n",
    "  #   if (c[1] <= 0.05):\n",
    "  #     num_fea_engineer_combine_selected_columns.append(fea)\n",
    "  #   # print(f'{props} \\np-value={c[1]}\\n')\n",
    "\n",
    "  # train_num_fea_engineer_combine = train_num_fea_engineer_combine[num_fea_engineer_combine_selected_columns]\n",
    "  # test_num_fea_engineer_combine = test_num_fea_engineer_combine[num_fea_engineer_combine_selected_columns] \n",
    "  \n",
    "  train_num_fea_engineer_combines.append(train_num_fea_engineer_combine)\n",
    "  test_num_fea_engineer_combines.append(test_num_fea_engineer_combine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zm42j4TPN-4F"
   },
   "source": [
    "# Mean Median Max Min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bus8FbzdOC3w"
   },
   "source": [
    "## KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wKAwYnolOI04",
    "outputId": "e35c0aa6-0770-4e2d-c242-1d033bfe928f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "numeric_features = list(set(train.columns) - set(cat_features))\n",
    "numeric_features.remove('label')\n",
    "train_cat_fea_engineer_stats_combines = []\n",
    "test_cat_fea_engineer_stats_combines = []\n",
    "for i in range(5):\n",
    "  train_cat_fea_engineer_stats_combine = pd.DataFrame()\n",
    "  test_cat_fea_engineer_stats_combine = pd.DataFrame()\n",
    "  for cat in cat_features:\n",
    "    for num in numeric_features:\n",
    "      # mean\n",
    "      train_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_train'][cat].map(\n",
    "          kfold[i]['X_train'].groupby(cat)[num].mean()\n",
    "      )\n",
    "      test_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_dev'][cat].map(\n",
    "          kfold[i]['X_dev'].groupby(cat)[num].mean()\n",
    "      )\n",
    "\n",
    "      # median\n",
    "      train_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_train'][cat].map(\n",
    "          kfold[i]['X_train'].groupby(cat)[num].median()\n",
    "      )\n",
    "      test_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_dev'][cat].map(\n",
    "          kfold[i]['X_dev'].groupby(cat)[num].median()\n",
    "      )\n",
    "\n",
    "      # min\n",
    "      train_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_train'][cat].map(\n",
    "          kfold[i]['X_train'].groupby(cat)[num].min()\n",
    "      )\n",
    "      test_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_dev'][cat].map(\n",
    "          kfold[i]['X_dev'].groupby(cat)[num].min()\n",
    "      )\n",
    "\n",
    "      # max\n",
    "      train_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_train'][cat].map(\n",
    "          kfold[i]['X_train'].groupby(cat)[num].max()\n",
    "      )\n",
    "      test_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_dev'][cat].map(\n",
    "          kfold[i]['X_dev'].groupby(cat)[num].max()\n",
    "      )\n",
    "  \n",
    "  num_fea_engineer_stats_combine_selected_columns = []\n",
    "  for fea in train_cat_fea_engineer_stats_combine:\n",
    "    props = pd.crosstab(train_cat_fea_engineer_stats_combine[fea], kfold[i]['y_train'])\n",
    "    c = chi2_contingency(props, lambda_='log-likelihood')\n",
    "    if (c[1] <= 0.05):\n",
    "      num_fea_engineer_stats_combine_selected_columns.append(fea)\n",
    "    # print(f'{props} \\np-value={c[1]}\\n')\n",
    "\n",
    "  train_cat_fea_engineer_stats_combine = train_cat_fea_engineer_stats_combine[num_fea_engineer_stats_combine_selected_columns]\n",
    "  test_cat_fea_engineer_stats_combine = test_cat_fea_engineer_stats_combine[num_fea_engineer_stats_combine_selected_columns] \n",
    "\n",
    "  train_cat_fea_engineer_stats_combines.append(train_cat_fea_engineer_stats_combine)\n",
    "  test_cat_fea_engineer_stats_combines.append(test_cat_fea_engineer_stats_combine)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9glacjg53I4A"
   },
   "source": [
    "# Weight of Evidence and information value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PDlVscu913p"
   },
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nrusPWRF913w",
    "outputId": "87729142-83dc-4ad3-de2d-5bbae608164b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5\n",
      "(24000, 1370)\n",
      "(6000, 1370)\n",
      "[INFO] filtering variables ...\n",
      "Variable filtering on 24000 rows and 1371 columns in 00:12:24 \n",
      "97 variables are removed\n",
      "[INFO] creating woe binning ...\n",
      ">>> There are 4 variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \n",
      "province, FIELD_13, maCv, district\n",
      ">>> Continue the binning process?\n",
      "1: yes \n",
      "2: no\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Selection:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning on 24000 rows and 1274 columns in 00:07:49\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 24000 rows and 1273 columns in 00:01:37\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 6000 rows and 1273 columns in 00:00:28\n",
      "2/5\n",
      "(24000, 1378)\n",
      "(6000, 1378)\n",
      "[INFO] filtering variables ...\n",
      "Variable filtering on 24000 rows and 1379 columns in 00:12:26 \n",
      "111 variables are removed\n",
      "[INFO] creating woe binning ...\n",
      ">>> There are 4 variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \n",
      "province, FIELD_13, maCv, district\n",
      ">>> Continue the binning process?\n",
      "1: yes \n",
      "2: no\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Selection:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning on 24000 rows and 1268 columns in 00:07:02\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 24000 rows and 1267 columns in 00:01:38\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 6000 rows and 1267 columns in 00:00:28\n",
      "3/5\n",
      "(24000, 1400)\n",
      "(6000, 1400)\n",
      "[INFO] filtering variables ...\n",
      "Variable filtering on 24000 rows and 1401 columns in 00:12:52 \n",
      "97 variables are removed\n",
      "[INFO] creating woe binning ...\n",
      ">>> There are 4 variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \n",
      "province, FIELD_13, maCv, district\n",
      ">>> Continue the binning process?\n",
      "1: yes \n",
      "2: no\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Selection:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning on 24000 rows and 1304 columns in 00:04:44\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 24000 rows and 1303 columns in 00:01:40\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 6000 rows and 1303 columns in 00:00:29\n",
      "4/5\n",
      "(24000, 1383)\n",
      "(6000, 1383)\n",
      "[INFO] filtering variables ...\n",
      "Variable filtering on 24000 rows and 1384 columns in 00:12:43 \n",
      "97 variables are removed\n",
      "[INFO] creating woe binning ...\n",
      ">>> There are 4 variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \n",
      "province, FIELD_13, maCv, district\n",
      ">>> Continue the binning process?\n",
      "1: yes \n",
      "2: no\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Selection:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning on 24000 rows and 1287 columns in 00:06:34\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 24000 rows and 1286 columns in 00:01:38\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 6000 rows and 1286 columns in 00:00:29\n",
      "5/5\n",
      "(24000, 1368)\n",
      "(6000, 1368)\n",
      "[INFO] filtering variables ...\n",
      "Variable filtering on 24000 rows and 1369 columns in 00:12:35 \n",
      "104 variables are removed\n",
      "[INFO] creating woe binning ...\n",
      ">>> There are 4 variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \n",
      "province, FIELD_13, maCv, district\n",
      ">>> Continue the binning process?\n",
      "1: yes \n",
      "2: no\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Selection:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning on 24000 rows and 1265 columns in 00:22:47\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 24000 rows and 1264 columns in 00:01:39\n",
      "[INFO] converting into woe values ...\n",
      "Woe transformating on 6000 rows and 1264 columns in 00:00:29\n",
      "time: 2h 2min 50s\n"
     ]
    }
   ],
   "source": [
    "import scorecardpy as sc\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "from scipy.stats import chi2_contingency\n",
    "import copy\n",
    "\n",
    "for i in range(len(kfold)):\n",
    "  print(f'{i+1}/{len(kfold)}')\n",
    "  # filter variable via missing rate\n",
    "  X_train = pd.concat([\n",
    "                                    kfold[i]['X_train'],               \n",
    "                                    train_cat_fea_engineer_combines[i],\n",
    "                                    train_num_fea_engineer_combines[i],\n",
    "                                    train_cat_fea_engineer_stats_combines[i]\n",
    "  ], axis=1)\n",
    "  print(X_train.shape)\n",
    "  X_dev = pd.concat([\n",
    "                                    kfold[i]['X_dev'],               \n",
    "                                    test_cat_fea_engineer_combines[i],\n",
    "                                    test_num_fea_engineer_combines[i],\n",
    "                                    test_cat_fea_engineer_stats_combines[i]\n",
    "  ], axis=1)\n",
    "  print(X_dev.shape)\n",
    "\n",
    "  dt_s = sc.var_filter(pd.concat([X_train, kfold[i]['y_train']], axis=1), y='label')\n",
    "  bins = sc.woebin(dt_s, y='label', bin_num_limit=20, positive=\"label|1\", method='tree')\n",
    "\n",
    "  train_woe = sc.woebin_ply(X_train, bins)\n",
    "\n",
    "\n",
    "  test_woe = sc.woebin_ply(X_dev, bins)\n",
    "  test_woe.reset_index(drop=True, inplace=True)\n",
    "  temp = train_woe.sample(test_woe.shape[0], replace=True)\n",
    "  temp.reset_index(drop=True, inplace=True)\n",
    "  test_woe.fillna(temp, inplace=True)\n",
    "\n",
    "  woe_selected_columns = []\n",
    "  for col in bins:\n",
    "    if bins[col]['total_iv'][0] > 0.1:\n",
    "      woe_selected_columns.append(col + '_woe')\n",
    "  # for fea in train_woe:\n",
    "  #   props = pd.crosstab(train_woe[fea], kfold[i]['y_train'])\n",
    "  #   c = chi2_contingency(props, lambda_='log-likelihood')\n",
    "  #   if (c[1] <= 0.05):\n",
    "  #     woe_selected_columns.append(fea)\n",
    "  #     # print(f'{props} \\np-value={c[1]}\\n')\n",
    "  train_woe = train_woe[woe_selected_columns]\n",
    "  test_woe = test_woe[woe_selected_columns]\n",
    "  \n",
    "\n",
    "  # train_count = pd.DataFrame()\n",
    "  # test_count = pd.DataFrame()\n",
    "  # train_woe['label'] = kfold[i]['y_train']\n",
    "  # for fea in train_woe.drop(columns=['label']):\n",
    "  #   groups = train_woe.groupby(fea, as_index=True)\n",
    "\n",
    "  #   bad = groups.sum()['label']\n",
    "  #   good = groups.count()['label'] - groups.sum()['label']\n",
    "  #   train_count[fea + '_N_bad'] = train_woe[fea].map(bad)\n",
    "  #   train_count[fea + '_N_good'] = train_woe[fea].map(good)\n",
    "\n",
    "  #   test_count[fea + '_N_bad'] = test_woe[fea].map(bad)\n",
    "  #   test_count[fea + '_N_good'] = test_woe[fea].map(good)\n",
    "  # train_woe.drop(columns=['label'], inplace=True)\n",
    "\n",
    "  # count_selected_columns = []\n",
    "  # for fea in train_count:\n",
    "  #   props = pd.crosstab(train_count[fea], train['label'])\n",
    "  #   c = chi2_contingency(props, lambda_='log-likelihood')\n",
    "  #   if (c[1] <= 0.05):\n",
    "  #     count_selected_columns.append(fea)\n",
    "  #     # print(f'{props} \\np-value={c[1]}\\n')\n",
    "\n",
    "  # train_count = train_count[count_selected_columns]\n",
    "  # test_count = test_count[count_selected_columns]\n",
    "  \n",
    "  train_dup_cols = set(X_train.columns).intersection(set(train_woe.columns))\n",
    "  test_dup_cols = copy.deepcopy(train_dup_cols)\n",
    "\n",
    "  X_train = X_train.drop(columns=cat_features)\n",
    "  X_dev = X_dev.drop(columns=cat_features)\n",
    "  \n",
    "  kfold[i]['X_train'] = pd.concat([X_train, train_woe.drop(columns=train_dup_cols)], axis=1)\n",
    "  kfold[i]['X_dev'] = pd.concat([X_dev, test_woe.drop(columns=test_dup_cols)], axis=1)\n",
    "  \n",
    "  del temp, dt_s, bins, train_woe, test_woe, X_train, X_dev\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IeboHW2914G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(kfold)):\n",
    "  # kfold[i][\"X_train\"].drop(columns=['label'], inplace=True)\n",
    "  kfold[i][\"X_train\"].to_csv(\"./cv_input/X_train_preprocessed_{}.csv\".format(i + 1), index=False)\n",
    "  kfold[i][\"X_dev\"].to_csv(\"./cv_input/X_dev_preprocessed_{}.csv\".format(i + 1), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSMpXkRTdCDZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 1392)\n",
      "(6000, 1392)\n",
      "(24000, 1381)\n",
      "(6000, 1381)\n",
      "(24000, 1444)\n",
      "(6000, 1444)\n",
      "(24000, 1409)\n",
      "(6000, 1409)\n",
      "(24000, 1403)\n",
      "(6000, 1403)\n",
      "time: 1.64 ms\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "  print(kfold[i]['X_train'].shape)\n",
    "  print(kfold[i]['X_dev'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BuSnMPmD57fT",
    "vtwH3-PduvD2"
   ],
   "name": "Credit Scoring Model - Data preprocessing - Feature Engineering - Feature Extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
