{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit Scoring Model - Data preprocessing - Feature Engineering - Feature Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BuSnMPmD57fT",
        "vtwH3-PduvD2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEbyXNrFr2pG",
        "colab_type": "code",
        "outputId": "6918ab80-3056-49af-8521-1faeac448095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/Thesis/Kalapa\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "time: 4.63 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmYMAvvVsIOP",
        "colab_type": "code",
        "outputId": "2091bfa1-cf64-40fa-d769-3a652869d46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "# numpy and pandas for data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# matplotlib and seaborn for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Garbage collector\n",
        "import gc\n",
        "\n",
        "!pip3 install unidecode\n",
        "!pip3 install ipython-autotime\n",
        "!pip3 install scorecardpy\n",
        "%load_ext autotime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: scorecardpy in /usr/local/lib/python3.6/dist-packages (0.1.9.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scorecardpy) (3.2.1)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.6/dist-packages (from scorecardpy) (1.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scorecardpy) (1.18.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scorecardpy) (0.22.2.post1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scorecardpy) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scorecardpy) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scorecardpy) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scorecardpy) (1.2.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.0->scorecardpy) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scorecardpy) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scorecardpy) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->scorecardpy) (1.12.0)\n",
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "time: 10.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ6kEDIhsNGo",
        "colab_type": "code",
        "outputId": "5d8b8c6b-2ba7-4f69-e9ba-153371ad714b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import gc\n",
        "gc.enable()\n",
        "\n",
        "def pca(train, test):\n",
        "    pca = PCA(n_components=0.97, svd_solver='full')\n",
        "    train_pca = pd.DataFrame(pca.fit_transform(train), columns = ['pca_' + str(i) for i in range(pca.n_components_)])\n",
        "    test_pca = pd.DataFrame(pca.transform(test), columns = ['pca_' + str(i) for i in range(pca.n_components_)])\n",
        "    return (train_pca, test_pca)\n",
        "\n",
        "def one_hot_encoding(X_train, X_test):\n",
        "    train = X_train.copy()\n",
        "    test = X_test.copy()\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "    train_ohe = pd.DataFrame()\n",
        "    test_ohe = pd.DataFrame()\n",
        "\n",
        "    for fea in train:\n",
        "        train[fea] = train[fea].replace(to_replace=[np.nan], value='none')\n",
        "        test[fea] = test[fea].replace(to_replace=[np.nan], value='none')\n",
        "\n",
        "        temp_train = enc.fit_transform(train[fea].values.reshape(-1,1)).toarray()\n",
        "        temp_test = enc.transform(test[fea].values.reshape(-1,1)).toarray()\n",
        "\n",
        "        train_ohe = pd.concat([train_ohe, pd.DataFrame(temp_train, columns=[fea + '_ohe_' + str(enc.categories_[0][i]) for i in range(len(enc.categories_[0]))])], axis=1)\n",
        "        test_ohe = pd.concat([test_ohe, pd.DataFrame(temp_test, columns=[fea + '_ohe_' + str(enc.categories_[0][i]) for i in range(len(enc.categories_[0]))])], axis=1)\n",
        "    del train, test\n",
        "    gc.collect()\n",
        "    return (train_ohe, test_ohe)\n",
        "\n",
        "def label_encoding(X_train, X_test):\n",
        "    train = X_train.copy()\n",
        "    test = X_test.copy()\n",
        "    \n",
        "    train_label = pd.DataFrame()\n",
        "    test_label = pd.DataFrame()\n",
        "\n",
        "    for fea in train:\n",
        "        train[fea] = train[fea].replace(to_replace=[np.nan], value='none')\n",
        "        test[fea] = test[fea].replace(to_replace=[np.nan], value='none')\n",
        "\n",
        "        factorised = pd.factorize(train[fea])[1]\n",
        "        labels = pd.Series(range(len(factorised)), index=factorised)\n",
        "\n",
        "        temp_train = train[fea].map(labels)\n",
        "        temp_test = test[fea].map(labels)\n",
        "\n",
        "        train_label[fea + '_labeled'] = temp_train\n",
        "        test_label[fea + '_labeled'] = temp_test\n",
        "\n",
        "    train_label.fillna(-1, inplace=True)\n",
        "    test_label.fillna(-1, inplace=True)\n",
        "    del train, test\n",
        "    gc.collect()\n",
        "    return (train_label, test_label)\n",
        "\n",
        "def freq_encoding(X_train, X_test):\n",
        "    train = X_train.copy()\n",
        "    test = X_test.copy()\n",
        "    \n",
        "    encoded_train_cols = dict()\n",
        "    encoded_test_cols = dict()\n",
        "    for col in train:\n",
        "        train[col] = train[col].replace(to_replace=[np.nan], value='none')\n",
        "        test[col] = test[col].replace(to_replace=[np.nan], value='none')\n",
        "\n",
        "        freq_cats = train.groupby([col])[col].count()/train.shape[0]\n",
        "        encoded_train_cols[str(col) + '_freq'] = train[col].map(freq_cats)\n",
        "        encoded_test_cols[str(col) + '_freq'] = test[col].map(freq_cats)\n",
        "\n",
        "    encoded_train_cols = pd.DataFrame(encoded_train_cols)\n",
        "    encoded_train_cols.fillna(0, inplace=True)\n",
        "    encoded_test_cols = pd.DataFrame(encoded_test_cols)\n",
        "    encoded_test_cols.fillna(0, inplace=True)\n",
        "    del train, test\n",
        "    gc.collect()\n",
        "    return (encoded_train_cols, encoded_test_cols)\n",
        "\n",
        "\n",
        "def mean_encoding(X_train, X_test, target, alpha=0, folds=5, random=True, random_state=913100):\n",
        "    \n",
        "    train = pd.concat([X_train, target], axis=1)\n",
        "    test = X_test.copy()\n",
        "    encoded_train_cols = dict()\n",
        "    encoded_test_cols = dict()\n",
        "    target_mean_gobal = train[target.name].mean()\n",
        "    \n",
        "    for col in X_train:\n",
        "      train[col] = train[col].replace(to_replace=[np.nan], value='none')\n",
        "      test[col] = test[col].replace(to_replace=[np.nan], value='none')\n",
        "\n",
        "      # Getting mean for test data\n",
        "      groups = train.groupby([col])\n",
        "      nrows_cat = groups[target.name].count()\n",
        "      target_mean_cats = groups[target.name].mean()\n",
        "      target_mean_cats_adj = (target_mean_cats*nrows_cat + target_mean_gobal*alpha) / (nrows_cat + alpha) \n",
        "      # Mapping mean to test data\n",
        "      encoded_test_cols[str(col) + '_mean'] = test[col].map(target_mean_cats_adj)\n",
        "\n",
        "      if folds is None:\n",
        "        encoded_train_cols[str(col) + '_mean'] = train[col].map(target_mean_cats_adj)\n",
        "      else:\n",
        "        kfold = StratifiedKFold(n_splits=folds, shuffle=random, random_state=random_state)  \n",
        "        parts = []\n",
        "        # Kfold for train data\n",
        "        for tr_idx, dev_idx in kfold.split(train.drop(columns=target.name), train[target.name]):\n",
        "            # Divide data\n",
        "            base_df, estimate_df = train.iloc[tr_idx], train.iloc[dev_idx]\n",
        "\n",
        "            # Gettting mean of base_df for estimation\n",
        "            groups = base_df.groupby([col])\n",
        "            nrows_cat = groups[target.name].count()\n",
        "            target_mean_cats = groups[target.name].mean()\n",
        "            target_mean_cats_adj = (target_mean_cats*nrows_cat + target_mean_gobal*alpha) / (nrows_cat + alpha) \n",
        "            # Mapping mran for estimate_df\n",
        "            parts.extend(estimate_df[col].map(target_mean_cats_adj))\n",
        "\n",
        "        encoded_train_cols[str(col)+ '_mean'] = parts\n",
        "  \n",
        "    encoded_train_cols = pd.DataFrame(encoded_train_cols)\n",
        "    encoded_train_cols.fillna(target_mean_gobal, inplace=True)\n",
        "    \n",
        "    encoded_test_cols = pd.DataFrame(encoded_test_cols)\n",
        "    encoded_test_cols.fillna(target_mean_gobal, inplace=True)\n",
        "    del train, test\n",
        "    gc.collect() \n",
        "    return (encoded_train_cols, encoded_test_cols)\n",
        "\n",
        "def scoring_ngboost_clf(X_train, y_train, X_dev, y_dev, random_state=913100, verbose=False):\n",
        "    iterations = []\n",
        "    train_scores = []\n",
        "    dev_scores = []\n",
        "  \n",
        "  \n",
        "    log_iters = list(set((np.logspace(math.log(1, 8), math.log(500, 8), \n",
        "                                        num=50, endpoint=True, base=8, \n",
        "                                        dtype=np.int))))\n",
        "    for estimators in sorted(log_iters):\n",
        "        model = MyNGBClassifier(n_estimators=estimators, random_state=random_state)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_train_pred_scores = model.predict_proba(X_train)\n",
        "        y_dev_pred_scores = model.predict_proba(X_dev)\n",
        "\n",
        "        train_scores.append(roc_auc_score(y_train, y_train_pred_scores[:, 1]))\n",
        "        dev_scores.append(roc_auc_score(y_dev, y_dev_pred_scores[:, 1]))\n",
        "        iterations.append(estimators)\n",
        "        if verbose:\n",
        "            print(f'{iterations[-1]}/{len(log_iters)}', train_scores[-1], dev_scores[-1])\n",
        "  \n",
        "    best_score = max(dev_scores)\n",
        "    best_iter = iterations[dev_scores.index(best_score)]\n",
        "    if verbose:\n",
        "        print(f'Best score: {best_score}. Best iter: {best_iter}')\n",
        "    return (train_scores, dev_scores, iterations, model)\n",
        "\n",
        "def test_all_encodings(train, dev, target_name):\n",
        "    # Format: encoding function, encoding params, encoding name, encoding color\n",
        "    encoding_settings = [\n",
        "                      [one_hot_encoding, {}, 'One hot encoding', '#E7E005'],\n",
        "                      [label_encoding, {}, 'Label encoding', '#960000'],\n",
        "                      [freq_encoding, {}, 'Frequency encoding', '#FF2F02'],\n",
        "                      [mean_encoding, {'alpha':0, 'folds':None, 'target':train['label']}, 'Mean encoding, alpha=0', '#A4C400'],\n",
        "                      [mean_encoding, {'alpha':2, 'folds':None, 'target':train['label']}, 'Mean encoding, alpha=2', '#73B100'],\n",
        "                      [mean_encoding, {'alpha':5, 'folds':None, 'target':train['label']}, 'Mean encoding, alpha=5', '#2B8E00'],\n",
        "                      [mean_encoding, {'alpha':5, 'folds':3, 'target':train['label']}, 'Mean encoding, alpha=5, 3 folds', '#00F5F2'],\n",
        "                      [mean_encoding, {'alpha':5, 'folds':5, 'target':train['label']}, 'Mean encoding, alpha=5, 5 folds', '#00BAD3'],\n",
        "    ]\n",
        "    scoring_func = scoring_ngboost_clf\n",
        "    plt.figure(figsize=(10,7))\n",
        "\n",
        "    review_rows = []\n",
        "\n",
        "    for encoding_func, encoding_params, str_name, color in encoding_settings:\n",
        "        print(str_name)\n",
        "        X_train, X_dev = encoding_func(train.drop(columns=target_name), dev.drop(columns=target_name), **encoding_params)\n",
        "\n",
        "        # X_train_pca, X_dev_pca = pca(X_train, X_dev)\n",
        "\n",
        "        scores = scoring_func(X_train, train[target_name], X_dev, dev[target_name])\n",
        "\n",
        "        train_scores, dev_scores, iters, _ = scores\n",
        "        plt.plot(iters,  dev_scores,  label='Test, ' + str_name, linewidth=1.5, color=color)\n",
        "\n",
        "        best_score_dev = max(dev_scores)\n",
        "        best_iter_dev = iters[dev_scores.index(best_score_dev)]\n",
        "        best_score_train = max(train_scores[:best_iter_dev])\n",
        "\n",
        "        print(f'Best score for {str_name} is {best_score_dev}, on estimators {best_iter_dev}')\n",
        "        review_rows.append([str_name, best_score_train, best_score_dev, best_iter_dev])\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "    columns = ['Encoding', 'Train AUC score on best iteration', 'Best AUC score (test)', 'Best iteration (test)']\n",
        "    return pd.DataFrame(review_rows, columns=columns)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 457 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbYc0FhpsSuy",
        "colab_type": "code",
        "outputId": "6c0d2e43-3c90-4649-f316-70981aac50fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train = pd.read_csv('./input/train_input_imputed.csv')\n",
        "test = pd.read_csv('./input/test_input_imputed.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 399 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIPBjScJ0Kc",
        "colab_type": "code",
        "outputId": "0ee1b1ae-9be3-42be-a276-641ae6fec601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.11 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nydc0kZm5q8W",
        "colab_type": "code",
        "outputId": "af0f8f05-181d-4825-d7d2-08d34ded6fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "kfold = []\n",
        "for i in range(0,5):\n",
        "  X_train = pd.read_csv(\"./cv_input/X_train_{}.csv\".format(i + 1))\n",
        "  y_train = pd.read_csv(\"./cv_input/y_train_{}.csv\".format(i + 1))\n",
        "  X_dev = pd.read_csv(\"./cv_input/X_dev_{}.csv\".format(i + 1))\n",
        "  y_dev = pd.read_csv(\"./cv_input/y_dev_{}.csv\".format(i + 1))\n",
        "  kfold.append({\n",
        "      \"X_train\": X_train,\n",
        "      \"y_train\": y_train['label'],\n",
        "      \"X_dev\": X_dev,\n",
        "      \"y_dev\": y_dev['label'],\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.57 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExpAuDPdpha8",
        "colab_type": "code",
        "outputId": "1aca68c8-f714-47fa-ac3c-c7fb73ffb83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "for i in range(0, 5):\n",
        "  print(kfold[i]['X_train'].shape)\n",
        "  print(kfold[i]['X_dev'].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24000, 63)\n",
            "(6000, 63)\n",
            "(24000, 63)\n",
            "(6000, 63)\n",
            "(24000, 63)\n",
            "(6000, 63)\n",
            "(24000, 63)\n",
            "(6000, 63)\n",
            "(24000, 63)\n",
            "(6000, 63)\n",
            "time: 2.12 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc3Ee0GHs24K",
        "colab_type": "text"
      },
      "source": [
        "# Categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA2GeV_3s6bd",
        "colab_type": "code",
        "outputId": "c8496fc9-2dfb-4095-972a-e5fe8a17c3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "cat_features = [col for col in train if train[col].dtype == 'object']\n",
        "cat_features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['province',\n",
              " 'district',\n",
              " 'maCv',\n",
              " 'FIELD_8',\n",
              " 'FIELD_9',\n",
              " 'FIELD_10',\n",
              " 'FIELD_13',\n",
              " 'FIELD_35',\n",
              " 'FIELD_39',\n",
              " 'FIELD_41',\n",
              " 'FIELD_42',\n",
              " 'FIELD_44']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "stream",
          "text": [
            "time: 10.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuSnMPmD57fT",
        "colab_type": "text"
      },
      "source": [
        "## Full dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiY6GEYAtLNy",
        "colab_type": "code",
        "outputId": "9c23b9da-e014-4785-c4c7-cd5bda51d385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# # # Transform FIELD_10 FIELD_13 FIELD_39\n",
        "# from unidecode import unidecode\n",
        "\n",
        "# for feature in ['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']:\n",
        "#   train[feature].replace(to_replace=[np.nan], value='none', inplace=True)\n",
        "#   train[feature] = train[feature].apply(unidecode).apply(str.lower)\n",
        "  \n",
        "#   test[feature].replace(to_replace=[np.nan], value='none', inplace=True)\n",
        "#   test[feature] = test[feature].apply(unidecode).apply(str.lower)\n",
        "\n",
        "# train_cat_ohe, test_cat_ohe = one_hot_encoding(train[['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
        "#                                                test[['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']])\n",
        "\n",
        "\n",
        "# train_cat_label, test_cat_label = label_encoding(train[['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
        "#                                                test[['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']])\n",
        "\n",
        "# train_cat_feq, test_cat_feq = freq_encoding(train[['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
        "#                                                test[['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']])\n",
        "\n",
        "# encoding_params =  {'alpha':5, 'folds':3, 'target':train['label']}\n",
        "# train_cat_mean, test_cat_mean = mean_encoding(train[['province', 'FIELD_8', 'FIELD_9', 'FIELD_10', 'FIELD_13', 'FIELD_35', 'FIELD_41', 'FIELD_42', 'FIELD_44']],\n",
        "#                                               test[['province', 'FIELD_8', 'FIELD_9', 'FIELD_10', 'FIELD_13', 'FIELD_35', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
        "#                                               **encoding_params) \n",
        "\n",
        "# train_cat_fea_engineer_combine = pd.concat([train_cat_ohe, train_cat_label, train_cat_feq, train_cat_mean], axis=1)\n",
        "# test_cat_fea_engineer_combine = pd.concat([test_cat_ohe, test_cat_label, test_cat_feq, test_cat_mean], axis=1)\n",
        "# del train_cat_ohe, train_cat_label, train_cat_feq, train_cat_mean, test_cat_ohe, test_cat_label, test_cat_feq, test_cat_mean\n",
        "# gc.collect()\n",
        "\n",
        "# from scipy.stats import chi2_contingency\n",
        "\n",
        "# selected_columns = []\n",
        "# chi2_check = []\n",
        "# unique_values = []\n",
        "# for fea in train_cat_fea_engineer_combine:\n",
        "#   unique_values.append(train_cat_fea_engineer_combine[fea].unique())\n",
        "#   props = pd.crosstab(train_cat_fea_engineer_combine[fea], train['label'])\n",
        "#   c = chi2_contingency(props, lambda_='log-likelihood')\n",
        "#   if (c[1] < 0.05):\n",
        "#     selected_columns.append(fea)\n",
        "#     chi2_check.append('Reject Null Hyposthesis')\n",
        "#   else:\n",
        "#     chi2_check.append('Failed to Reject Null Hyposthesis')\n",
        "#   # print(f'{props} \\np-value={c[1]}\\n')\n",
        "# res = pd.DataFrame(data = {'Feature':train_cat_fea_engineer_combine.columns, \n",
        "#                            'Unique values': unique_values, \n",
        "#                            'Hypothesis': chi2_check})\n",
        "\n",
        "# train = pd.concat([train,train_cat_fea_engineer_combine[selected_columns]], axis=1)\n",
        "# test = pd.concat([test,test_cat_fea_engineer_combine[selected_columns]], axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 11.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ascb3F6iU4I",
        "colab_type": "code",
        "outputId": "560eeaf8-81de-4f4c-b86e-4a06a5f3f55b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 754 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_f-XlpX5-Av",
        "colab_type": "text"
      },
      "source": [
        "## KFOLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo9gk9Uz5_i-",
        "colab_type": "code",
        "outputId": "5480aeee-ce49-4032-c3ba-5f40a7c610dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# # Transform FIELD_10 FIELD_13 FIELD_39\n",
        "from unidecode import unidecode\n",
        "from scipy.stats import chi2_contingency\n",
        "train_cat_fea_engineer_combines = []\n",
        "test_cat_fea_engineer_combines = []\n",
        "for i in range(len(kfold)):\n",
        "  for feature in ['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']:\n",
        "    kfold[i]['X_train'][feature].replace(to_replace=[np.nan], value='none', inplace=True)\n",
        "    kfold[i]['X_train'][feature] = kfold[i]['X_train'][feature].apply(unidecode).apply(str.lower)\n",
        "\n",
        "    kfold[i]['X_dev'][feature].replace(to_replace=[np.nan], value='none', inplace=True)\n",
        "    kfold[i]['X_dev'][feature] = kfold[i]['X_dev'][feature].apply(unidecode).apply(str.lower)\n",
        "\n",
        "  train_cat_ohe, test_cat_ohe = one_hot_encoding(kfold[i]['X_train'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
        "                                                 kfold[i]['X_dev'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']])\n",
        "\n",
        "\n",
        "  train_cat_label, test_cat_label = label_encoding(kfold[i]['X_train'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
        "                                                  kfold[i]['X_dev'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']])\n",
        "\n",
        "  train_cat_feq, test_cat_feq = freq_encoding(kfold[i]['X_train'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
        "                                              kfold[i]['X_dev'][['province', 'FIELD_8', 'FIELD_9','FIELD_10','FIELD_13','FIELD_35', 'FIELD_39', 'FIELD_41', 'FIELD_42', 'FIELD_44']])\n",
        "\n",
        "  encoding_params =  {'alpha':5, 'folds':3, 'target':kfold[i]['y_train']}\n",
        "  train_cat_mean, test_cat_mean = mean_encoding(kfold[i]['X_train'][['province', 'FIELD_8', 'FIELD_9', 'FIELD_10', 'FIELD_13', 'FIELD_35', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
        "                                                kfold[i]['X_dev'][['province', 'FIELD_8', 'FIELD_9', 'FIELD_10', 'FIELD_13', 'FIELD_35', 'FIELD_41', 'FIELD_42', 'FIELD_44']], \n",
        "                                                **encoding_params) \n",
        "\n",
        "\n",
        "  train_cat_fea_engineer_combine = pd.concat([train_cat_ohe, train_cat_label, train_cat_feq, train_cat_mean], axis=1)\n",
        "  test_cat_fea_engineer_combine = pd.concat([test_cat_ohe, test_cat_label, test_cat_feq, test_cat_mean], axis=1)\n",
        "\n",
        "  \n",
        "\n",
        "  del train_cat_ohe, train_cat_label, train_cat_feq, train_cat_mean, test_cat_ohe, test_cat_label, test_cat_feq, test_cat_mean\n",
        "  gc.collect()\n",
        "\n",
        "  cat_fea_engineer_combine_selected_columns = []\n",
        "  for fea in train_cat_fea_engineer_combine:\n",
        "    props = pd.crosstab(train_cat_fea_engineer_combine[fea], kfold[i]['y_train'])\n",
        "    c = chi2_contingency(props, lambda_='log-likelihood')\n",
        "    if (c[1] <= 0.05):\n",
        "      cat_fea_engineer_combine_selected_columns.append(fea)\n",
        "    # print(f'{props} \\np-value={c[1]}\\n')\n",
        "  train_cat_fea_engineer_combine = train_cat_fea_engineer_combine[cat_fea_engineer_combine_selected_columns]\n",
        "  test_cat_fea_engineer_combine = test_cat_fea_engineer_combine[cat_fea_engineer_combine_selected_columns] \n",
        "\n",
        "  train_cat_fea_engineer_combines.append(train_cat_fea_engineer_combine)\n",
        "  test_cat_fea_engineer_combines.append(test_cat_fea_engineer_combine)\n",
        "  # kfold[i]['X_train'] = pd.concat([kfold[i]['X_train'], train_cat_fea_engineer_combines[cat_fea_engineer_combine_selected_columns]], axis=1)\n",
        "  # kfold[i]['X_dev'] = pd.concat([kfold[i]['X_dev'], train_cat_fea_engineer_combines[cat_fea_engineer_combine_selected_columns]], axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 38.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sji9LuCj6kF0",
        "colab_type": "text"
      },
      "source": [
        "# Add Subtract Divide Multiply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oErhJQWxLs88",
        "colab_type": "text"
      },
      "source": [
        "## KFOLD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNJ_D6D26oPK",
        "colab_type": "code",
        "outputId": "ea5abef2-d890-4278-86fb-b4e4c9f7c92c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "auto_columns = 'FIELD_1 FIELD_2 FIELD_3 FIELD_4 FIELD_5 FIELD_6 FIELD_14 FIELD_15 FIELD_16 FIELD_21 FIELD_22 FIELD_32 FIELD_33 FIELD_34 FIELD_46 FIELD_50 \\\n",
        "FIELD_51 FIELD_52 FIELD_53 FIELD_54 FIELD_55 FIELD_56 FIELD_57'.split()\n",
        "train_num_fea_engineer_combines = []\n",
        "test_num_fea_engineer_combines = []\n",
        "for i in range(len(kfold)):\n",
        "  train_num_fea_engineer_combine = pd.DataFrame()\n",
        "  test_num_fea_engineer_combine = pd.DataFrame()\n",
        "  for l, r in combinations(auto_columns, 2):  \n",
        "    for func in 'add subtract divide multiply'.split():\n",
        "      train_num_fea_engineer_combine[f'auto_{func}_{l}_{r}'] = getattr(np, func)(kfold[i]['X_train'][l], kfold[i]['X_train'][r])\n",
        "      test_num_fea_engineer_combine[f'auto_{func}_{l}_{r}'] = getattr(np, func)(kfold[i]['X_dev'][l], kfold[i]['X_dev'][r])\n",
        "  \n",
        "  train_num_fea_engineer_combine.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "  train_num_fea_engineer_combine.fillna(-999, inplace=True)\n",
        "\n",
        "  test_num_fea_engineer_combine.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "  test_num_fea_engineer_combine.fillna(-999, inplace=True)\n",
        "  \n",
        "\n",
        "  # num_fea_engineer_combine_selected_columns = []\n",
        "  # for fea in train_num_fea_engineer_combine:\n",
        "  #   props = pd.crosstab(train_num_fea_engineer_combine[fea], kfold[i]['y_train'])\n",
        "  #   c = chi2_contingency(props, lambda_='log-likelihood')\n",
        "  #   if (c[1] <= 0.05):\n",
        "  #     num_fea_engineer_combine_selected_columns.append(fea)\n",
        "  #   # print(f'{props} \\np-value={c[1]}\\n')\n",
        "\n",
        "  # train_num_fea_engineer_combine = train_num_fea_engineer_combine[num_fea_engineer_combine_selected_columns]\n",
        "  # test_num_fea_engineer_combine = test_num_fea_engineer_combine[num_fea_engineer_combine_selected_columns] \n",
        "  \n",
        "  train_num_fea_engineer_combines.append(train_num_fea_engineer_combine)\n",
        "  test_num_fea_engineer_combines.append(test_num_fea_engineer_combine)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 14.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm42j4TPN-4F",
        "colab_type": "text"
      },
      "source": [
        "# Mean Median Max Min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bus8FbzdOC3w",
        "colab_type": "text"
      },
      "source": [
        "## KFOLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKAwYnolOI04",
        "colab_type": "code",
        "outputId": "e35c0aa6-0770-4e2d-c242-1d033bfe928f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "numeric_features = list(set(train.columns) - set(cat_features))\n",
        "numeric_features.remove('label')\n",
        "train_cat_fea_engineer_stats_combines = []\n",
        "test_cat_fea_engineer_stats_combines = []\n",
        "for i in range(5):\n",
        "  train_cat_fea_engineer_stats_combine = pd.DataFrame()\n",
        "  test_cat_fea_engineer_stats_combine = pd.DataFrame()\n",
        "  for cat in cat_features:\n",
        "    for num in numeric_features:\n",
        "      # mean\n",
        "      train_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_train'][cat].map(\n",
        "          kfold[i]['X_train'].groupby(cat)[num].mean()\n",
        "      )\n",
        "      test_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_dev'][cat].map(\n",
        "          kfold[i]['X_dev'].groupby(cat)[num].mean()\n",
        "      )\n",
        "\n",
        "      # median\n",
        "      train_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_train'][cat].map(\n",
        "          kfold[i]['X_train'].groupby(cat)[num].median()\n",
        "      )\n",
        "      test_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_dev'][cat].map(\n",
        "          kfold[i]['X_dev'].groupby(cat)[num].median()\n",
        "      )\n",
        "\n",
        "      # min\n",
        "      train_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_train'][cat].map(\n",
        "          kfold[i]['X_train'].groupby(cat)[num].min()\n",
        "      )\n",
        "      test_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_dev'][cat].map(\n",
        "          kfold[i]['X_dev'].groupby(cat)[num].min()\n",
        "      )\n",
        "\n",
        "      # max\n",
        "      train_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_train'][cat].map(\n",
        "          kfold[i]['X_train'].groupby(cat)[num].max()\n",
        "      )\n",
        "      test_cat_fea_engineer_stats_combine[f'stats_mean_{cat}_{num}'] = kfold[i]['X_dev'][cat].map(\n",
        "          kfold[i]['X_dev'].groupby(cat)[num].max()\n",
        "      )\n",
        "  \n",
        "  num_fea_engineer_stats_combine_selected_columns = []\n",
        "  for fea in train_cat_fea_engineer_stats_combine:\n",
        "    props = pd.crosstab(train_cat_fea_engineer_stats_combine[fea], kfold[i]['y_train'])\n",
        "    c = chi2_contingency(props, lambda_='log-likelihood')\n",
        "    if (c[1] <= 0.05):\n",
        "      num_fea_engineer_stats_combine_selected_columns.append(fea)\n",
        "    # print(f'{props} \\np-value={c[1]}\\n')\n",
        "\n",
        "  train_cat_fea_engineer_stats_combine = train_cat_fea_engineer_stats_combine[num_fea_engineer_stats_combine_selected_columns]\n",
        "  test_cat_fea_engineer_stats_combine = test_cat_fea_engineer_stats_combine[num_fea_engineer_stats_combine_selected_columns] \n",
        "\n",
        "  train_cat_fea_engineer_stats_combines.append(train_cat_fea_engineer_stats_combine)\n",
        "  test_cat_fea_engineer_stats_combines.append(test_cat_fea_engineer_stats_combine)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9glacjg53I4A",
        "colab_type": "text"
      },
      "source": [
        "# Weight of Evidence and information value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtwH3-PduvD2",
        "colab_type": "text"
      },
      "source": [
        "## Full dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8MnSR0S3Kky",
        "colab_type": "code",
        "outputId": "ae7d113e-9f67-40c9-8eb0-0d82ddfae830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# import scorecardpy as sc\n",
        "# from pylab import rcParams\n",
        "# rcParams['figure.figsize'] = 20, 10\n",
        "# # filter variable via missing rate\n",
        "# dt_s = sc.var_filter(train, y='label')\n",
        "\n",
        "# bins = sc.woebin(dt_s, y='label', bin_num_limit=20, positive=\"label|1\", method='tree')\n",
        "\n",
        "# train_woe = sc.woebin_ply(train.drop(columns=['label']), bins)\n",
        "# train_woe['label'] = train['label']\n",
        "\n",
        "# test_woe = sc.woebin_ply(test, bins)\n",
        "# test_woe.reset_index(drop=True, inplace=True)\n",
        "# temp = train_woe.sample(test_woe.shape[0], replace=True).drop(columns='label')\n",
        "# temp.reset_index(drop=True, inplace=True)\n",
        "# test_woe.fillna(temp, inplace=True)\n",
        "\n",
        "# del temp\n",
        "# gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.74 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F2GcnJ3KTrn",
        "colab_type": "code",
        "outputId": "6297326c-221d-4f02-d307-66c661d3f128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# from scipy.stats import chi2_contingency\n",
        "\n",
        "# selected_columns = []\n",
        "# for fea in train_woe.drop(columns='label'):\n",
        "#   props = pd.crosstab(train_woe[fea], train['label'])\n",
        "#   c = chi2_contingency(props, lambda_='log-likelihood')\n",
        "#   if (c[1] <= 0.05):\n",
        "#     selected_columns.append(fea)\n",
        "#     # print(f'{props} \\np-value={c[1]}\\n')\n",
        "# train_woe = train_woe[selected_columns]\n",
        "# train_woe['label'] = train['label']\n",
        "# test_woe = test_woe[selected_columns]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.35 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHEsdJy6iuZr",
        "colab_type": "code",
        "outputId": "e61afe99-d9ef-4070-c0a9-0f089edf0d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# train_woe.to_csv('./input/train_woe_input.csv')\n",
        "# test_woe.to_csv('./input/test_woe_input.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.72 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6z2LQ06607d",
        "colab_type": "code",
        "outputId": "08f96c18-ff37-44fe-aeb7-6cdfbd0f6405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# train_count = pd.DataFrame()\n",
        "# test_count = pd.DataFrame()\n",
        "# for fea in train_woe.drop(columns=['label']):\n",
        "#   groups = train_woe.groupby(fea, as_index=True)\n",
        "\n",
        "#   bad = groups.sum()['label']\n",
        "#   good = groups.count()['label'] - groups.sum()['label']\n",
        "#   train_count[fea + '_N_bad'] = train_woe[fea].map(bad)\n",
        "#   train_count[fea + '_N_good'] = train_woe[fea].map(good)\n",
        "  \n",
        "#   test_count[fea + '_N_bad'] = test_woe[fea].map(bad)\n",
        "#   test_count[fea + '_N_good'] = test_woe[fea].map(good)\n",
        "# from scipy.stats import chi2_contingency\n",
        "\n",
        "# selected_columns = []\n",
        "# for fea in train_count:\n",
        "#   props = pd.crosstab(train_count[fea], train['label'])\n",
        "#   c = chi2_contingency(props, lambda_='log-likelihood')\n",
        "#   if (c[1] <= 0.05):\n",
        "#     selected_columns.append(fea)\n",
        "#     # print(f'{props} \\np-value={c[1]}\\n')\n",
        "\n",
        "# train_count = train_count[selected_columns]\n",
        "# test_count = test_count[selected_columns]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 5.17 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZkd8Npt77WH",
        "colab_type": "code",
        "outputId": "a6fcaa08-a1c2-4f0e-a12a-d092617d3ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# import copy\n",
        "# train_dup_cols = set(train.columns).intersection(set(train_woe.columns))\n",
        "# test_dup_cols = copy.deepcopy(train_dup_cols)\n",
        "# test_dup_cols.remove('label')\n",
        "\n",
        "# train = train.drop(columns=cat_features)\n",
        "# test = test.drop(columns=cat_features)\n",
        "\n",
        "# train_save = pd.concat([train, train_woe.drop(columns=train_dup_cols), train_count], axis=1)\n",
        "# test_save = pd.concat([test, test_woe.drop(columns=test_dup_cols), test_count], axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.01 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laN83-FzsKEC",
        "colab_type": "code",
        "outputId": "23193019-cd9c-4874-e2a1-5e961e4e5eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# train_save"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 738 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moSLyvR0cM0z",
        "colab_type": "code",
        "outputId": "a19e74a3-82e7-40b7-c553-8c8508f9829e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# train_save.to_csv('./input/train_input_preprocessed.csv', index=False)\n",
        "# test_save.to_csv('./input/test_input_preprocessed.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.04 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0PDlVscu913p"
      },
      "source": [
        "## KFold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "87729142-83dc-4ad3-de2d-5bbae608164b",
        "id": "nrusPWRF913w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import scorecardpy as sc\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 20, 10\n",
        "from scipy.stats import chi2_contingency\n",
        "import copy\n",
        "\n",
        "for i in range(len(kfold)):\n",
        "  print(f'{i+1}/{len(kfold)}')\n",
        "  # filter variable via missing rate\n",
        "  X_train = pd.concat([\n",
        "                                    kfold[i]['X_train'],               \n",
        "                                    train_cat_fea_engineer_combines[i],\n",
        "                                    train_num_fea_engineer_combines[i],\n",
        "                                    train_cat_fea_engineer_stats_combines[i]\n",
        "  ], axis=1)\n",
        "  print(X_train.shape)\n",
        "  X_dev = pd.concat([\n",
        "                                    kfold[i]['X_dev'],               \n",
        "                                    test_cat_fea_engineer_combines[i],\n",
        "                                    test_num_fea_engineer_combines[i],\n",
        "                                    test_cat_fea_engineer_stats_combines[i]\n",
        "  ], axis=1)\n",
        "  print(X_dev.shape)\n",
        "\n",
        "  dt_s = sc.var_filter(pd.concat([X_train, kfold[i]['y_train']], axis=1), y='label')\n",
        "  bins = sc.woebin(dt_s, y='label', bin_num_limit=20, positive=\"label|1\", method='tree')\n",
        "\n",
        "  train_woe = sc.woebin_ply(X_train, bins)\n",
        "\n",
        "\n",
        "  test_woe = sc.woebin_ply(X_dev, bins)\n",
        "  test_woe.reset_index(drop=True, inplace=True)\n",
        "  temp = train_woe.sample(test_woe.shape[0], replace=True)\n",
        "  temp.reset_index(drop=True, inplace=True)\n",
        "  test_woe.fillna(temp, inplace=True)\n",
        "\n",
        "  woe_selected_columns = []\n",
        "  for col in bins:\n",
        "    if bins[col]['total_iv'][0] > 0.1:\n",
        "      woe_selected_columns.append(col + '_woe')\n",
        "  # for fea in train_woe:\n",
        "  #   props = pd.crosstab(train_woe[fea], kfold[i]['y_train'])\n",
        "  #   c = chi2_contingency(props, lambda_='log-likelihood')\n",
        "  #   if (c[1] <= 0.05):\n",
        "  #     woe_selected_columns.append(fea)\n",
        "  #     # print(f'{props} \\np-value={c[1]}\\n')\n",
        "  train_woe = train_woe[woe_selected_columns]\n",
        "  test_woe = test_woe[woe_selected_columns]\n",
        "  \n",
        "\n",
        "  # train_count = pd.DataFrame()\n",
        "  # test_count = pd.DataFrame()\n",
        "  # train_woe['label'] = kfold[i]['y_train']\n",
        "  # for fea in train_woe.drop(columns=['label']):\n",
        "  #   groups = train_woe.groupby(fea, as_index=True)\n",
        "\n",
        "  #   bad = groups.sum()['label']\n",
        "  #   good = groups.count()['label'] - groups.sum()['label']\n",
        "  #   train_count[fea + '_N_bad'] = train_woe[fea].map(bad)\n",
        "  #   train_count[fea + '_N_good'] = train_woe[fea].map(good)\n",
        "\n",
        "  #   test_count[fea + '_N_bad'] = test_woe[fea].map(bad)\n",
        "  #   test_count[fea + '_N_good'] = test_woe[fea].map(good)\n",
        "  # train_woe.drop(columns=['label'], inplace=True)\n",
        "\n",
        "  # count_selected_columns = []\n",
        "  # for fea in train_count:\n",
        "  #   props = pd.crosstab(train_count[fea], train['label'])\n",
        "  #   c = chi2_contingency(props, lambda_='log-likelihood')\n",
        "  #   if (c[1] <= 0.05):\n",
        "  #     count_selected_columns.append(fea)\n",
        "  #     # print(f'{props} \\np-value={c[1]}\\n')\n",
        "\n",
        "  # train_count = train_count[count_selected_columns]\n",
        "  # test_count = test_count[count_selected_columns]\n",
        "  \n",
        "  train_dup_cols = set(X_train.columns).intersection(set(train_woe.columns))\n",
        "  test_dup_cols = copy.deepcopy(train_dup_cols)\n",
        "\n",
        "  X_train = X_train.drop(columns=cat_features)\n",
        "  X_dev = X_dev.drop(columns=cat_features)\n",
        "  \n",
        "  kfold[i]['X_train'] = pd.concat([X_train, train_woe.drop(columns=train_dup_cols)], axis=1)\n",
        "  kfold[i]['X_dev'] = pd.concat([X_dev, test_woe.drop(columns=test_dup_cols)], axis=1)\n",
        "  \n",
        "  del temp, dt_s, bins, train_woe, test_woe, X_train, X_dev\n",
        "  gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/5\n",
            "(24000, 1370)\n",
            "(6000, 1370)\n",
            "[INFO] filtering variables ...\n",
            "Variable filtering on 24000 rows and 1371 columns in 00:14:24 \n",
            "97 variables are removed\n",
            "[INFO] creating woe binning ...\n",
            ">>> There are 4 variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \n",
            "maCv, FIELD_13, district, province\n",
            ">>> Continue the binning process?\n",
            "1: yes \n",
            "2: no\n",
            "Binning on 24000 rows and 1274 columns in 00:16:27\n",
            "[INFO] converting into woe values ...\n",
            "Woe transformating on 24000 rows and 1273 columns in 00:04:28\n",
            "[INFO] converting into woe values ...\n",
            "Woe transformating on 6000 rows and 1273 columns in 00:01:33\n",
            "2/5\n",
            "(24000, 1378)\n",
            "(6000, 1378)\n",
            "[INFO] filtering variables ...\n",
            "Variable filtering on 24000 rows and 1379 columns in 00:14:23 \n",
            "111 variables are removed\n",
            "[INFO] creating woe binning ...\n",
            ">>> There are 4 variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \n",
            "maCv, FIELD_13, district, province\n",
            ">>> Continue the binning process?\n",
            "1: yes \n",
            "2: no\n",
            "Binning on 24000 rows and 1268 columns in 00:18:19\n",
            "[INFO] converting into woe values ...\n",
            "Woe transformating on 24000 rows and 1267 columns in 00:04:31\n",
            "[INFO] converting into woe values ...\n",
            "Woe transformating on 6000 rows and 1267 columns in 00:01:36\n",
            "3/5\n",
            "(24000, 1400)\n",
            "(6000, 1400)\n",
            "[INFO] filtering variables ...\n",
            "Variable filtering on 24000 rows and 1401 columns in 00:14:35 \n",
            "97 variables are removed\n",
            "[INFO] creating woe binning ...\n",
            ">>> There are 4 variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \n",
            "maCv, FIELD_13, district, province\n",
            ">>> Continue the binning process?\n",
            "1: yes \n",
            "2: no\n",
            "Binning on 24000 rows and 1304 columns in 00:13:18\n",
            "[INFO] converting into woe values ...\n",
            "Woe transformating on 24000 rows and 1303 columns in 00:04:42\n",
            "[INFO] converting into woe values ...\n",
            "Woe transformating on 6000 rows and 1303 columns in 00:01:40\n",
            "4/5\n",
            "(24000, 1383)\n",
            "(6000, 1383)\n",
            "[INFO] filtering variables ...\n",
            "Variable filtering on 24000 rows and 1384 columns in 00:14:41 \n",
            "97 variables are removed\n",
            "[INFO] creating woe binning ...\n",
            ">>> There are 4 variables have too many unique non-numberic values, which might cause the binning process slow. Please double check the following variables: \n",
            "maCv, FIELD_13, district, province\n",
            ">>> Continue the binning process?\n",
            "1: yes \n",
            "2: no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7IeboHW2914G",
        "colab": {}
      },
      "source": [
        "for i in range(0, len(kfold)):\n",
        "  # kfold[i][\"X_train\"].drop(columns=['label'], inplace=True)\n",
        "  kfold[i][\"X_train\"].to_csv(\"./cv_input/X_train_preprocessed_{}.csv\".format(i + 1), index=False)\n",
        "  kfold[i][\"X_dev\"].to_csv(\"./cv_input/X_dev_preprocessed_{}.csv\".format(i + 1), index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qSMpXkRTdCDZ",
        "colab": {}
      },
      "source": [
        "for i in range(0, 5):\n",
        "  print(kfold[i]['X_train'].shape)\n",
        "  print(kfold[i]['X_dev'].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}