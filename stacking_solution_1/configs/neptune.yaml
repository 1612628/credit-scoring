project: kapala-credit-scoring

name: kapala-credit-scoring
tag: [solution-1, stacking]

metric:
  channel: 'ROC_AUC'
  goal: maximize

exclude:
  - output
  - notebooks
  - neptune.log
  - offline_job.log
  - .git
  - .github
  - .idea
  - .ipynb_checkpoints

parameters:
# Data
  train_filepath: './input/train_input_imputed.csv'
  test_filepath: './input/test_input_imputed.csv'
  first_level_oof_predictions_dir: './models/stacking_1/oof'
  experiment_dir: './models/stacking_1/exp'
# Data preparation
  n_cv_splits: 5
  dev_size: 0.2
  stratified_cv: True
  shuffle: True
  hyperparameter_search__method: None
  hyperparameter_search__runs: 0

# Excution
  clean_experiment_directory_before_training: 1
  num_workers: 8
  verbose: 1
  callbacks_on: 1

# Preprocessing
  fill_missing: False
  fill_value: 0

# Feature extraction
  use_nan_count: True

# Feature selection
  use_train: True

# Light GBM
  lgbm__device: cpu
  lgbm__boosting_type: goss
  lgbm__objective: binary
  lgbm__metric: auc
  lgbm__number_boosting_rounds: 500
  lgbm__early_stopping_rounds: 50
  lgbm__learning_rate: 0.1
  lgbm__max_bin: 300
  lgbm__max_depth: 4
  lgbm__num_leaves: 644
  lgbm__min_child_samples: 330
  lgbm__subsample: 1.0
  lgbm__subsample_freq: 1
  lgbm__colsample_bytree: 0.377
  lgbm__min_gain_to_split: 3.405
  lgbm__reg_lambda: 0.0
  lgbm__reg_alpha: 0.0
  lgbm__is_unbalanced: True
  lgbm__scale_pos_weight: 1.
  
# Catboost
  catboost__loss_function: Logloss
  catboost__eval_metric: AUC
  catboost__iterations: 1000
  catboost__learning_rate: 0.05
  catboost__depth: 6
  catboost__l2_leaf_reg: 3
  catboost__border_count: 128
  catboost__model_size_reg: 0.5
  catboost__colsample_bylevel: 1.0
  catboost__max_ctr_complexity: 1
  catboost__od_type: 'Iter'
  catboost__od_wait: 100

# XGBoost
  xgb__booster: gbtree
  xgb__tree_method: hist # gpu_hist  # auto  hist
  xgb__objective: binary:logistic
  xgb__eval_metric: auc
  xgb__nrounds: 500
  xgb__early_stopping_rounds: 50
  xgb__eta: 0.1
  xgb__max_leaves: 4
  xgb__max_depth: 2
  xgb__max_bin: 300
  xgb__subsample: 0.8
  xgb__colsample_bytree: 0.3
  xgb__colsample_bylevel: 1
  xgb__min_child_weight: 9
  xgb__lambda: 0
  xgb__alpha: 100
  xgb__scale_pos_weight: 1

# Neural Network
  nn__layers: 2
  nn__neurons: '[16, 1]'
  nn__activation: '["relu", "sigmoid"]'
  nn__dropout: '[0., 0.]'
  nn__batch_norm: '[True, False]'
  nn__l1: '[0., 0.]'
  nn__l2: '[0., 0.]'
  nn__learning_rate: 0.001
  nn__beta_1: 0.9
  nn__beta_2: 0.99
  nn__epochs: 10
  nn__batch_size: 32

# Random forest
  rf__n_estimators: 200
  rf__criterion: gini
  rf__max_features: 0.2
  rf__max_depth: 40
  rf__min_samples_split: 50
  rf__min_samples_leaf: 20
  rf__max_leaf_nodes: 60
  rf__class_weight: balanced

# Logistic regression
  lr__penalty: l1
  lr__tol: 0.00001
  lr__C: 0.1
  lr__fit_intercept: 1
  lr__class_weight: balanced
  lr__solver: liblinear
  lr__max_iter: 500

# SVC
  svc__kernel: rbf
  svc__C: 1
  svc__degree: 5
  svc__gamma: auto
  svc__coef0: 0.0
  svc__probability: True
  svc__tol: 0.00001
  svc__max_iter: -1

# Naive Bayes
  nb__alpha: 1
  nb__binarize: 0